{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f6ee6e2-6c9d-4f1f-b833-618241119ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "You may need to run this cell the first time that you work with the code provided here. \n",
    "Uncomment the following line and run it.\n",
    "After the first time you run it, you should not have to run it again.\n",
    "You may have to restart Jupyter after installing to get everything up and running\n",
    "'''\n",
    "\n",
    "#%pip install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a0a5221-514b-4d50-a786-651afcd82f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run this cellblock to import all the necessary libraries and packages for the code.\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "#from google.colab import files\n",
    "import io\n",
    "\n",
    "# Download necessary NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64973b9d-007c-47e1-b6bc-203a0950b0fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================================\n",
    "# PART 0: CREATE FUNCTIONS TO UPLOAD AND LOAD TEXT DATA\n",
    "# ========================================================\n",
    "\n",
    "'''\n",
    "No inputs necessary here, just run this code block.\n",
    "'''\n",
    "\n",
    "# Function to load a CSV file (works in any Python environment)\n",
    "def load_csv(file_path=None):\n",
    "    \"\"\"\n",
    "    Load a CSV file from a file path and return a pandas DataFrame\n",
    "    \n",
    "    Parameters:\n",
    "    file_path (str, optional): Path to the CSV file. If None, will prompt user.\n",
    "    \n",
    "    Returns:\n",
    "    DataFrame: The loaded pandas DataFrame\n",
    "    \"\"\"\n",
    "    # Try to load the CSV from the file path\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "        print(f\"Successfully loaded '{file_path}' with {df.shape[0]} rows and {df.shape[1]} columns.\")\n",
    "        \n",
    "        # Display the first few rows to check the data\n",
    "        print(\"\\nPreview of your data:\")\n",
    "        print(df.head())\n",
    "        \n",
    "        # Display column names\n",
    "        print(\"\\nColumn names (you'll need these to select text columns):\")\n",
    "        for i, col in enumerate(df.columns):\n",
    "            print(f\"{i}: {col}\")\n",
    "            \n",
    "        return df\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error reading CSV file: {e}\")\n",
    "        return None\n",
    "\n",
    "# Function to extract text from a dataframe\n",
    "def extract_text_from_df(df, column_name, rows=None):\n",
    "    \"\"\"\n",
    "    Extract text from a specific column in the dataframe\n",
    "    \n",
    "    Parameters:\n",
    "    df (DataFrame): The pandas DataFrame\n",
    "    column_name (str): The name of the column containing text\n",
    "    rows (list or None): List of row indices to include, or None for all rows\n",
    "    \n",
    "    Returns:\n",
    "    str: Combined text from the specified column\n",
    "    \"\"\"\n",
    "    if column_name not in df.columns:\n",
    "        print(f\"Error: Column '{column_name}' not found in the dataframe.\")\n",
    "        print(f\"Available columns are: {', '.join(df.columns)}\")\n",
    "        return \"\"\n",
    "    \n",
    "    if rows is None:\n",
    "        # Use all rows if not specified\n",
    "        selected_data = df[column_name]\n",
    "    else:\n",
    "        # Use only specified rows\n",
    "        selected_data = df.loc[rows, column_name]\n",
    "    \n",
    "    # Filter out nan values and convert to string\n",
    "    text_data = [str(text) for text in selected_data if not pd.isna(text)]\n",
    "    \n",
    "    # Combine all text with newlines in between\n",
    "    combined_text = \"\\n\".join(text_data)\n",
    "    \n",
    "    print(f\"Extracted {len(text_data)} text entries from column '{column_name}'\")\n",
    "    print(f\"Total length: {len(combined_text)} characters\")\n",
    "    \n",
    "    return combined_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5093968-3b3a-4fb9-815a-99d2963ca0a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================================\n",
    "# PART 0: YOU UPLOAD AND LOAD TEXT DATA\n",
    "# ========================================================\n",
    "\n",
    "'''\n",
    "This is where you will upload the csv file with the tweets. \n",
    "If you are successful in uploading the data, you will see a message\n",
    "    Sucessfully loaded '___.csv' with 3467 rows and two columns    \n",
    "'''\n",
    "\n",
    "# Replace with your file path or run without arguments for prompt\n",
    "df = load_csv('???.csv')  \n",
    "\n",
    "\n",
    "# Change '???' to the actual column name containing your text data\n",
    "if df is not None:\n",
    "    text_column_name = '???'\n",
    "    sample_text = extract_text_from_df(df, text_column_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae4f8ff7-b0bc-47fa-b4df-39da7a4715a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================================\n",
    "# PART 1: TEXT PREPROCESSING FUNCTIONS\n",
    "# ========================================================\n",
    "\n",
    "'''\n",
    "Read through the code here to understand what the inputs are to the function \n",
    "and what you as a user can change. Then run this code block. No inputs are required from you\n",
    "here\n",
    "'''\n",
    "\n",
    "# Function to clean and preprocess text\n",
    "def preprocess_text(text, lowercase=True, remove_punctuation=True, remove_numbers=True):\n",
    "    \"\"\"\n",
    "    Clean and preprocess text by:\n",
    "    - Converting to lowercase (optional)\n",
    "    - Removing punctuation (optional)\n",
    "    - Removing numbers (optional)\n",
    "    \"\"\"\n",
    "    # Convert to lowercase\n",
    "    if lowercase:\n",
    "        text = text.lower()\n",
    "    \n",
    "    # Remove punctuation\n",
    "    if remove_punctuation:\n",
    "        text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    \n",
    "    # Remove numbers\n",
    "    if remove_numbers:\n",
    "        text = re.sub(r'\\d+', '', text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62eed8ec-bdab-42c7-8bab-1e80b8446914",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================================\n",
    "# PART 1: TEXT PREPROCESSING EXPLORATION\n",
    "# ========================================================\n",
    "\n",
    "'''\n",
    "Work with the function below and explore the different parameters.\n",
    "What does each of them appear to do to the output? How might this be helpful?\n",
    "'''\n",
    "preprocessed_text = preprocess_text(\n",
    "    sample_text, \n",
    "    lowercase=True,            \n",
    "    remove_punctuation=True,   \n",
    "    remove_numbers=True        \n",
    ")\n",
    "\n",
    "print(\"--- Preprocessed Text ---\")\n",
    "print(preprocessed_text[:500] + \"...\")  # Print first 500 characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a6801e9-1240-4942-b051-2ad4c1ea3e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================================\n",
    "# PART 2: TOKENIZATION FUNCTIONS\n",
    "# ========================================================\n",
    "\n",
    "# Function to tokenize text into sentences and words\n",
    "def tokenize_text(text):\n",
    "    \"\"\"\n",
    "    Split text into sentences and words\n",
    "    \"\"\"\n",
    "    # Tokenize into sentences\n",
    "    sentences = sent_tokenize(text)\n",
    "    \n",
    "    # Tokenize into words\n",
    "    words = word_tokenize(text)\n",
    "    \n",
    "    return sentences, words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1685f09-0c4e-4c1d-9e72-48e29c1340b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================================\n",
    "# PART 2: TOKENIZATION EXPLORATION\n",
    "# ========================================================\n",
    "\n",
    "'''\n",
    "Here you will complete several different tasks to get an understanding of the tokenization function\n",
    "\n",
    "    1. Return to part one (text preprocessing) and set all of the parameters to be true, run this code block\n",
    "    2. Return to part one (text preprocessing) and play around with the parameters to determine how many total sentences\n",
    "       there were in the tweets.\n",
    "    \n",
    "'''\n",
    "\n",
    "sentences, words = tokenize_text(preprocessed_text)\n",
    "\n",
    "print(f\"\\n--- Text Tokenization ---\")\n",
    "print(f\"Number of sentences: {len(sentences)}\")\n",
    "print(f\"Number of words: {len(words)}\")\n",
    "print(f\"\\nFirst 3 sentences:\")\n",
    "for i, sentence in enumerate(sentences[:3]):\n",
    "    print(f\"  {i+1}. {sentence}\")\n",
    "\n",
    "print(f\"\\nFirst 20 words:\")\n",
    "print(words[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e069c30e-dc7c-46da-8ca7-702a7890dc39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================================\n",
    "# PART 3: STOPWORD REMOVAL FUNCTION\n",
    "# ========================================================\n",
    "\n",
    "'''\n",
    "No inputs necessary here, just run this code block.\n",
    "'''\n",
    "\n",
    "def remove_stopwords(word_list, custom_stopwords=None):\n",
    "    \"\"\"\n",
    "    Remove common stopwords from a list of words\n",
    "    \"\"\"\n",
    "    # Get standard English stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    # Add custom stopwords if provided\n",
    "    if custom_stopwords:\n",
    "        stop_words.update(custom_stopwords)\n",
    "    \n",
    "    # Remove stopwords\n",
    "    filtered_words = [word for word in word_list if word not in stop_words]\n",
    "    \n",
    "    return filtered_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c10dc39d-1488-4e2d-85b2-134a217a2852",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================================\n",
    "# PART 3: STOPWORD REMOVAL EXPLORATION\n",
    "# ========================================================\n",
    "\n",
    "'''\n",
    "Using your favorite search engine look up what a stopword is in NLP.\n",
    "\n",
    "Once you understand what a stop word is run the code as it is written here.\n",
    "How many words were removed?\n",
    "\n",
    "Now add some of your own stop words. \n",
    "What words can you add to remove the largest number of words?\n",
    "'''\n",
    "\n",
    "custom_stopwords = ['', '', '' ]  #Add your stop words to this list here\n",
    "\n",
    "filtered_words = remove_stopwords(words, custom_stopwords)\n",
    "\n",
    "print(f\"\\n--- After Stopword Removal ---\")\n",
    "print(f\"Original word count: {len(words)}\")\n",
    "print(f\"Filtered word count: {len(filtered_words)}\")\n",
    "print(f\"Removed {len(words) - len(filtered_words)} stopwords\")\n",
    "print(f\"\\nFirst 20 filtered words:\")\n",
    "print(filtered_words[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb4ee95-886b-4210-8647-73b11874d8c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================================\n",
    "# PART 4: STEMMING AND LEMMATIZATION FUNCTION\n",
    "# ========================================================\n",
    "\n",
    "'''\n",
    "No inputs necessary here, just run this code block.\n",
    "'''\n",
    "\n",
    "# Function to perform stemming and lemmatization\n",
    "def stem_and_lemmatize(word_list):\n",
    "    \"\"\"\n",
    "    Apply stemming and lemmatization to a list of words\n",
    "    \"\"\"\n",
    "    stemmer = PorterStemmer()\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    stemmed_words = [stemmer.stem(word) for word in word_list]\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word) for word in word_list]\n",
    "    \n",
    "    return stemmed_words, lemmatized_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db06c48-4434-4d33-bc45-fadc8c47ba27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================================\n",
    "# PART 4: STEMMING AND LEMMATIZATION EXPLORATION\n",
    "# ========================================================\n",
    "\n",
    "'''\n",
    "Using your favorite search engine look up what stemming and lemmatization are in NLP.\n",
    "Why are these used? \n",
    "\n",
    "Once you understand what these methods are, run the following code and look at the output.\n",
    "What do you think the output tells us about tweets with regards to language complexity?\n",
    "'''\n",
    "\n",
    "stemmed_words, lemmatized_words = stem_and_lemmatize(filtered_words)\n",
    "\n",
    "print(f\"\\n--- Stemming vs Lemmatization ---\")\n",
    "print(f\"Original word count: {len(filtered_words)}\")\n",
    "\n",
    "# Show comparison for first 10 words\n",
    "print(f\"\\nComparison of first 10 words:\")\n",
    "print(f\"{'Original':<15} {'Stemmed':<15} {'Lemmatized':<15}\")\n",
    "print(\"-\" * 45)\n",
    "for i in range(10):\n",
    "    if i < len(filtered_words):\n",
    "        print(f\"{filtered_words[i]:<15} {stemmed_words[i]:<15} {lemmatized_words[i]:<15}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b2d522-1007-4dc8-b21e-189612d67c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================================\n",
    "# PART 5: FREQUENCY ANALYSIS FUNCTION\n",
    "# ========================================================\n",
    "\n",
    "'''\n",
    "No inputs necessary here, just run this code block.\n",
    "'''\n",
    "\n",
    "# Function to analyze word frequency\n",
    "def analyze_frequency(word_list, n=10):\n",
    "    \"\"\"\n",
    "    Find the most common words and plot frequency distribution\n",
    "    \"\"\"\n",
    "    # Get frequency distribution\n",
    "    fdist = FreqDist(word_list)\n",
    "    \n",
    "    # Get most common words\n",
    "    most_common = fdist.most_common(n)\n",
    "    \n",
    "    # Create lists for plotting\n",
    "    words_mc = [word for word, count in most_common]\n",
    "    counts_mc = [count for word, count in most_common]\n",
    "    \n",
    "    return fdist, words_mc, counts_mc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "996621ec-6c79-49fd-825b-bd72e26a34ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================================\n",
    "# PART 5: FREQUENCY ANALYSIS EXPLORATION\n",
    "# ========================================================\n",
    "\n",
    "'''\n",
    "Run this function and look at the output.\n",
    "\n",
    "Based on what you see, head back to previous parts of the code to \n",
    "create a frequency list, bar graph and word cloud that most accurately \n",
    "reflect the tweets. \n",
    "'''\n",
    "\n",
    "# Change the number to see more or fewer top words\n",
    "top_n = 15  # Number of top words to display\n",
    "\n",
    "fdist, top_words, top_counts = analyze_frequency(filtered_words, top_n)\n",
    "\n",
    "print(f\"\\n--- Word Frequency Analysis ---\")\n",
    "print(f\"Top {top_n} most frequent words:\")\n",
    "for word, count in zip(top_words, top_counts):\n",
    "    print(f\"  {word}: {count}\")\n",
    "\n",
    "# Plot frequency distribution\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(top_words, top_counts)\n",
    "plt.title(f'Top {top_n} Most Frequent Words')\n",
    "plt.xlabel('Words')\n",
    "plt.ylabel('Frequency')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Create word cloud\n",
    "wordcloud = WordCloud(width=800, height=400, background_color='white', \n",
    "                     max_words=100, contour_width=3, contour_color='steelblue')\n",
    "wordcloud.generate(' '.join(filtered_words))\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.title('Word Cloud of Frequent Terms')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27997fb4-d13f-40b1-b567-64ffb8849abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================================\n",
    "# PART 6: SENTIMENT ANALYSIS FUNCTION\n",
    "# ========================================================\n",
    "\n",
    "'''\n",
    "No inputs necessary here, just run this code block.\n",
    "'''\n",
    "\n",
    "# Function to analyze sentiment\n",
    "def analyze_sentiment(text, by_sentence=False):\n",
    "    \"\"\"\n",
    "    Perform sentiment analysis on text using VADER\n",
    "    If by_sentence is True, analyze each sentence separately\n",
    "    \"\"\"\n",
    "    # Initialize sentiment analyzer\n",
    "    sia = SentimentIntensityAnalyzer()\n",
    "    \n",
    "    if by_sentence:\n",
    "        # Analyze each sentence\n",
    "        sentences = sent_tokenize(text)\n",
    "        results = []\n",
    "        \n",
    "        for sentence in sentences:\n",
    "            sentiment = sia.polarity_scores(sentence)\n",
    "            results.append((sentence, sentiment))\n",
    "        \n",
    "        return results\n",
    "    else:\n",
    "        # Analyze entire text\n",
    "        sentiment = sia.polarity_scores(text)\n",
    "        return sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "039d917f-3457-4324-a12c-144c38b2d602",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================================\n",
    "# PART 6: SENTIMENT ANALYSIS EXPLORATION\n",
    "# ========================================================\n",
    "\n",
    "'''\n",
    "Run this block of code as it is to see the output.\n",
    "\n",
    "Now change analyze by sentence to True. What happens?\n",
    "'''\n",
    "\n",
    "analyze_by_sentence = False  \n",
    "\n",
    "sentiment_results = analyze_sentiment(sample_text, analyze_by_sentence)\n",
    "\n",
    "print(f\"\\n--- Sentiment Analysis ---\")\n",
    "if analyze_by_sentence:\n",
    "    print(\"Sentiment analysis by sentence:\")\n",
    "    for i, (sentence, sentiment) in enumerate(sentiment_results[:5]):  # Show first 5 for brevity\n",
    "        print(f\"\\nSentence {i+1}: {sentence}\")\n",
    "        print(f\"  Negative: {sentiment['neg']:.3f}\")\n",
    "        print(f\"  Neutral: {sentiment['neu']:.3f}\")\n",
    "        print(f\"  Positive: {sentiment['pos']:.3f}\")\n",
    "        print(f\"  Compound: {sentiment['compound']:.3f}\")\n",
    "    \n",
    "    # Also show average compound score\n",
    "    avg_compound = sum(s['compound'] for _, s in sentiment_results) / len(sentiment_results)\n",
    "    print(f\"\\nAverage compound score: {avg_compound:.3f}\")\n",
    "else:\n",
    "    print(\"Overall sentiment analysis:\")\n",
    "    print(f\"  Negative: {sentiment_results['neg']:.3f}\")\n",
    "    print(f\"  Neutral: {sentiment_results['neu']:.3f}\")\n",
    "    print(f\"  Positive: {sentiment_results['pos']:.3f}\")\n",
    "    print(f\"  Compound: {sentiment_results['compound']:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
